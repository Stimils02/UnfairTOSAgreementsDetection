# -*- coding: utf-8 -*-
"""balanced_bert_train_scraper_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EpqfTiXYWZqWtlag0F7wDCMsIO-QZTcJ
"""

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_dataset
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, random_split
import torch.nn as nn
import torch.optim as optim
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt

from datasets import load_dataset, Dataset
import pandas as pd

# Load full dataset
raw_dataset = load_dataset("LawInformedAI/claudette_tos")["train"]
df = pd.DataFrame(raw_dataset)

# Print original label distribution
print("Original label distribution:\n", df["label"].value_counts())

# Balance the dataset
n_samples = min(df["label"].value_counts().values)
df_balanced = pd.concat([
    df[df["label"] == 0].sample(n=n_samples, random_state=42),
    df[df["label"] == 1].sample(n=n_samples, random_state=42)
]).sample(frac=1, random_state=42).reset_index(drop=True)

# Print balanced label distribution
print("Balanced label distribution:\n", df_balanced["label"].value_counts())

# Convert back to Hugging Face Dataset
dataset = Dataset.from_pandas(df_balanced)
print(f"Balanced dataset size: {len(dataset)}")

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokens = tokenizer(list(dataset["text"]), padding=True, truncation=True, return_tensors="pt")
labels = torch.tensor(dataset["label"])

data_tensor = TensorDataset(tokens["input_ids"], tokens["attention_mask"], labels)

train_size = int(0.8 * len(data_tensor))
val_size = len(data_tensor) - train_size
train_dataset, val_dataset = random_split(data_tensor, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=8, sampler=RandomSampler(train_dataset))
val_loader = DataLoader(val_dataset, batch_size=8, sampler=SequentialSampler(val_dataset))

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2).to(device)

optimizer = optim.AdamW(model.parameters(), lr=2e-5)
loss_fn = nn.CrossEntropyLoss()

for epoch in range(3):
    model.train()
    total_loss = 0
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}")

    for batch in progress_bar:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = loss_fn(outputs.logits, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

        progress_bar.set_description(f"Epoch {epoch+1} Loss: {loss.item():.4f}")

    print(f"Epoch {epoch+1} Avg Loss: {total_loss / len(train_loader):.4f}")

model.eval()
y_true, y_pred = [], []

with torch.no_grad():
    for batch in val_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)

        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

print(f"Validation Accuracy: {accuracy_score(y_true, y_pred):.4f}")
print(f"F1 Score: {f1_score(y_true, y_pred):.4f}")
print(f"Precision: {precision_score(y_true, y_pred):.4f}")
print(f"Recall: {recall_score(y_true, y_pred):.4f}")

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

model.save_pretrained("bert_full_model")
tokenizer.save_pretrained("bert_full_model")
print("✅ Model and tokenizer saved to 'bert_full_model/'")

"""Scraper data"""

from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

# ✅ Load from local folder, not from Hugging Face Hub
model = AutoModelForSequenceClassification.from_pretrained(
    "bert_full_model", local_files_only=True
)
tokenizer = AutoTokenizer.from_pretrained(
    "bert_full_model", local_files_only=True
)

# ✅ Move to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()  # inference mode

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import torch
from torch.nn.functional import softmax
from transformers import AutoTokenizer

# Load CSV
file_path = "/content/drive/MyDrive/Sahithi/filtered_uk_terms_all_months.csv"
df = pd.read_csv(file_path)

# Check for the expected column
if "terms_content" not in df.columns:
    raise ValueError("The column 'terms_content' is not found in the dataset.")

# Tokenize input
encoded = tokenizer.batch_encode_plus(
    df["terms_content"].tolist(),
    padding=True,
    truncation=True,
    return_tensors="pt"
)

# Move inputs to the same device as model
encoded = {k: v.to(model.device) for k, v in encoded.items()}

# Run inference
with torch.no_grad():
    outputs = model(**encoded)
    probs = softmax(outputs.logits, dim=1)
    preds = torch.argmax(probs, dim=1).cpu().numpy()

# Append to DataFrame
df["predicted_label"] = preds

# Show sample predictions
df[["terms_content", "predicted_label"]].head()

# Show all rows where the model predicted class 1
df[df["predicted_label"] == 1][["terms_content", "predicted_label"]]

# Show distribution of predicted labels
df["predicted_label"].value_counts()

# Visualize the distribution
import matplotlib.pyplot as plt

df["predicted_label"].value_counts().plot(kind="bar", color=["skyblue", "salmon"])
plt.title("Distribution of Predicted Labels")
plt.xlabel("Predicted Label")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.show()

df[["website_url", "predicted_label"]]